{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L2IJsRYD2a_B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def zeros_initialization(shape):\n",
        "    return np.zeros(shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_initialization(shape):\n",
        "    return np.random.randn(*shape) * 0.01"
      ],
      "metadata": {
        "id": "KmHwuiNH2hxX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_initialization(shape):\n",
        "    xavier_limit = np.sqrt(6 / (shape[0] + shape[1]))\n",
        "    return np.random.uniform(low=-xavier_limit, high=xavier_limit, size=shape)"
      ],
      "metadata": {
        "id": "eM5jJDId2lFs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def he_initialization(shape):\n",
        "    he_limit = np.sqrt(2 / shape[0])\n",
        "    return np.random.randn(*shape) * he_limit"
      ],
      "metadata": {
        "id": "ZPiT6yr42oci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_size = 100\n",
        "hidden_size = 50\n",
        "output_size = 10\n",
        "\n",
        "# Initialize weights and biases using different methods\n",
        "W1 = zeros_initialization((hidden_size, input_size))\n",
        "b1 = zeros_initialization((hidden_size, 1))\n",
        "\n",
        "W2 = random_initialization((output_size, hidden_size))\n",
        "b2 = random_initialization((output_size, 1))\n",
        "\n",
        "W3 = xavier_initialization((hidden_size, input_size))\n",
        "b3 = xavier_initialization((hidden_size, 1))\n",
        "\n",
        "W4 = he_initialization((output_size, hidden_size))\n",
        "b4 = he_initialization((output_size, 1))"
      ],
      "metadata": {
        "id": "EqYE7JH82rM2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "PuPHXlqy2zQt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(z):\n",
        "    return np.tanh(z)"
      ],
      "metadata": {
        "id": "jY0d0Q0O279z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)"
      ],
      "metadata": {
        "id": "zuexD8d42_Hc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha * z, z)"
      ],
      "metadata": {
        "id": "hHcLFEIg3Bos"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "z = np.array([-2, -1, 0, 1, 2])\n",
        "\n",
        "# Apply activation functions to the input\n",
        "sigmoid_output = sigmoid(z)\n",
        "tanh_output = tanh(z)\n",
        "relu_output = relu(z)\n",
        "leaky_relu_output = leaky_relu(z)\n",
        "\n",
        "print(\"Sigmoid output:\", sigmoid_output)\n",
        "print(\"Tanh output:\", tanh_output)\n",
        "print(\"ReLU output:\", relu_output)\n",
        "print(\"Leaky ReLU output:\", leaky_relu_output)"
      ],
      "metadata": {
        "id": "zgVvqqb63EsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from time import time\n",
        "\n",
        "# Define the activation functions\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha * z, z)\n",
        "\n",
        "# Define the neural network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, initialization, activation):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.initialization = initialization\n",
        "        self.activation = activation\n",
        "        self.weights = None\n",
        "        self.biases = None\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        if self.initialization == 'zeros':\n",
        "            self.weights = np.zeros((self.hidden_size, self.input_size))\n",
        "            self.biases = np.zeros((self.hidden_size, 1))\n",
        "        elif self.initialization == 'random':\n",
        "            self.weights = np.random.randn(self.hidden_size, self.input_size) * 0.01\n",
        "            self.biases = np.random.randn(self.hidden_size, 1) * 0.01\n",
        "        elif self.initialization == 'xavier':\n",
        "            xavier_limit = np.sqrt(6 / (self.input_size + self.hidden_size))\n",
        "            self.weights = np.random.uniform(low=-xavier_limit, high=xavier_limit,\n",
        "                                             size=(self.hidden_size, self.input_size))\n",
        "            self.biases = np.random.uniform(low=-xavier_limit, high=xavier_limit,\n",
        "                                            size=(self.hidden_size, 1))\n",
        "        elif self.initialization == 'he':\n",
        "            he_limit = np.sqrt(2 / self.input_size)\n",
        "            self.weights = np.random.randn(self.hidden_size, self.input_size) * he_limit\n",
        "            self.biases = np.random.randn(self.hidden_size, 1) * he_limit\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        z = np.dot(self.weights, X) + self.biases\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid(z)\n",
        "        elif self.activation == 'tanh':\n",
        "            return tanh(z)\n",
        "        elif self.activation == 'relu':\n",
        "            return relu(z)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return leaky_relu(z)\n",
        "\n",
        "    def train(self, X_train, y_train, learning_rate, num_epochs):\n",
        "        self.initialize_weights()\n",
        "        m = X_train.shape[1]\n",
        "        costs = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Forward propagation\n",
        "            A = self.forward_propagation(X_train)\n",
        "\n",
        "            # Compute cost\n",
        "            cost = -np.sum(y_train * np.log(A) + (1 - y_train) * np.log(1 - A)) / m\n",
        "            costs.append(cost)\n",
        "\n",
        "            # Backward propagation\n",
        "            dZ = A - y_train\n",
        "            dW = np.dot(dZ, X_train.T) / m\n",
        "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= learning_rate * dW\n",
        "            self.biases -= learning_rate * db\n",
        "\n",
        "        return costs\n",
        "\n",
        "    def predict(self, X):\n",
        "        A = self.forward_propagation(X)\n",
        "        return (A > 0.5).astype(int)\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the list of initialization methods and activation functions to try\n",
        "initializations = ['zeros', 'random', 'xavier', 'he']\n",
        "activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
        "\n",
        "# Perform experiments\n",
        "results = []\n",
        "for initialization in initializations:\n",
        "    for activation in activations:\n",
        "        start_time = time()\n",
        "        # Create and train the neural network\n",
        "        model = NeuralNetwork(input_size=X.shape[1], hidden_size=10, output_size=1,\n",
        "                              initialization=initialization, activation=activation)\n",
        "        costs = model.train(X_train.T, y_train.reshape(1, -1), learning_rate=0.01, num_epochs=100)\n",
        "        # Evaluate the model on the test set\n",
        "        y_pred = model.predict(X_test.T)\n",
        "        accuracy = accuracy_score(y_test, y_pred.flatten())\n",
        "        convergence_time = time() - start_time\n",
        "        results.append((initialization, activation, accuracy, convergence_time))\n",
        "\n",
        "# Display the results\n",
        "print(\"Results:\")\n",
        "for initialization, activation, accuracy, convergence_time in results:\n",
        "    print(f\"Initialization: {initialization}, Activation: {activation}, \"\n",
        "          f\"Accuracy: {accuracy:.4f}, Convergence Time: {convergence_time:.2f} seconds\")\n",
        "\n",
        "# Plot the cost curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "for initialization, activation, _, _ in results:\n",
        "    costs = [cost for cost in costs if cost is not None]\n",
        "    plt.plot(range(len(costs)), costs, label=f\"{initialization}_{activation}\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Cost Curves for Different Initialization Methods and Activation Functions\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "hXnGsQY33HZp",
        "outputId": "33b4aefe-a86b-45f4-ef05-d9ded380d880"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c974ca031ad4>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mconvergence_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 2000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3BDOgRs3tAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}