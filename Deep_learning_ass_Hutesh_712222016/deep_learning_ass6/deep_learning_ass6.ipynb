{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "k35syiv2Bkl_",
        "outputId": "91a12394-8d9c-41fe-d4e1-abff205f9df5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a154ed70c54>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# Split the dataset into train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Create an instance of the SigmoidNeuron class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SigmoidNeuron:\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def initialize_parameters(self, num_features):\n",
        "        self.weights = np.zeros((num_features, 1))\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def compute_cost(self, A, y):\n",
        "        m = y.shape[0]\n",
        "        cost = -np.sum(y * np.log(A) + (1 - y) * np.log(1 - A)) / m\n",
        "        return cost\n",
        "\n",
        "    def backward_propagation(self, X, A, y):\n",
        "        m = y.shape[0]\n",
        "        dZ = A - y\n",
        "        dW = np.dot(X.T, dZ) / m\n",
        "        dB = np.sum(dZ) / m\n",
        "        return dW, dB\n",
        "\n",
        "    def update_parameters(self, dW, dB, learning_rate):\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.bias -= learning_rate * dB\n",
        "\n",
        "    def train(self, X, y, num_iterations, learning_rate, optimizer='gd', batch_size=None,\n",
        "              beta=0.9, epsilon=1e-8, beta1=0.9, beta2=0.999):\n",
        "        m = X.shape[0]\n",
        "        costs = []\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.initialize_parameters(X.shape[1])\n",
        "\n",
        "        # Optimization algorithms\n",
        "        if optimizer == 'gd':  # Gradient Descent\n",
        "            for i in range(num_iterations):\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                self.update_parameters(dW, dB, learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'mbgd':  # Mini-Batch Gradient Descent\n",
        "            num_batches = m // batch_size\n",
        "            for i in range(num_iterations):\n",
        "                for j in range(num_batches):\n",
        "                    start = j * batch_size\n",
        "                    end = start + batch_size\n",
        "                    X_batch = X[start:end]\n",
        "                    y_batch = y[start:end]\n",
        "                    A = self.forward_propagation(X_batch)\n",
        "                    cost = self.compute_cost(A, y_batch)\n",
        "                    dW, dB = self.backward_propagation(X_batch, A, y_batch)\n",
        "                    self.update_parameters(dW, dB, learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'momentum':  # Momentum-based Gradient Descent\n",
        "            v_w = np.zeros_like(self.weights)\n",
        "            v_b = 0\n",
        "            for i in range(num_iterations):\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                v_w = beta * v_w + (1 - beta) * dW\n",
        "                v_b = beta * v_b + (1 - beta) * dB\n",
        "                self.update_parameters(v_w, v_b, learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'nag':  # Nesterov Accelerated Gradient (NAG)\n",
        "            v_w = np.zeros_like(self.weights)\n",
        "            v_b = 0\n",
        "            for i in range(num_iterations):\n",
        "                # Save current parameters\n",
        "                curr_weights = self.weights.copy()\n",
        "                curr_bias = self.bias\n",
        "\n",
        "                # Update parameters with momentum\n",
        "                self.weights -= beta * v_w\n",
        "                self.bias -= beta * v_b\n",
        "\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                v_w = beta * v_w + (1 - beta) * dW\n",
        "                v_b = beta * v_b + (1 - beta) * dB\n",
        "\n",
        "                # Restore original parameters\n",
        "                self.weights = curr_weights - learning_rate * v_w\n",
        "                self.bias = curr_bias - learning_rate * v_b\n",
        "\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'adagrad':  # Adaptive Gradients (AdaGrad)\n",
        "            cache_w = np.zeros_like(self.weights)\n",
        "            cache_b = 0\n",
        "            for i in range(num_iterations):\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                cache_w += dW**2\n",
        "                cache_b += dB**2\n",
        "                self.update_parameters(dW / (np.sqrt(cache_w) + epsilon),\n",
        "                                       dB / (np.sqrt(cache_b) + epsilon),\n",
        "                                       learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'rmsprop':  # Root Mean Squared Propagation (RMSProp)\n",
        "            cache_w = np.zeros_like(self.weights)\n",
        "            cache_b = 0\n",
        "            for i in range(num_iterations):\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                cache_w = beta * cache_w + (1 - beta) * dW**2\n",
        "                cache_b = beta * cache_b + (1 - beta) * dB**2\n",
        "                self.update_parameters(dW / (np.sqrt(cache_w) + epsilon),\n",
        "                                       dB / (np.sqrt(cache_b) + epsilon),\n",
        "                                       learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        elif optimizer == 'adam':  # Adam Optimization\n",
        "            v_w = np.zeros_like(self.weights)\n",
        "            v_b = 0\n",
        "            s_w = np.zeros_like(self.weights)\n",
        "            s_b = 0\n",
        "            t = 0\n",
        "            for i in range(num_iterations):\n",
        "                t += 1\n",
        "                A = self.forward_propagation(X)\n",
        "                cost = self.compute_cost(A, y)\n",
        "                dW, dB = self.backward_propagation(X, A, y)\n",
        "                v_w = beta1 * v_w + (1 - beta1) * dW\n",
        "                v_b = beta1 * v_b + (1 - beta1) * dB\n",
        "                s_w = beta2 * s_w + (1 - beta2) * dW**2\n",
        "                s_b = beta2 * s_b + (1 - beta2) * dB**2\n",
        "                v_w_corrected = v_w / (1 - beta1**t)\n",
        "                v_b_corrected = v_b / (1 - beta1**t)\n",
        "                s_w_corrected = s_w / (1 - beta2**t)\n",
        "                s_b_corrected = s_b / (1 - beta2**t)\n",
        "                self.update_parameters(v_w_corrected / (np.sqrt(s_w_corrected) + epsilon),\n",
        "                                       v_b_corrected / (np.sqrt(s_b_corrected) + epsilon),\n",
        "                                       learning_rate)\n",
        "                costs.append(cost)\n",
        "\n",
        "        return costs\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Generate a sample dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 10)\n",
        "y = np.random.randint(0, 2, size=(100, 1))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the SigmoidNeuron class\n",
        "neuron = SigmoidNeuron()\n",
        "\n",
        "# Train the neuron using different optimization algorithms\n",
        "gd_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='gd')\n",
        "mbgd_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='mbgd', batch_size=16)\n",
        "momentum_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='momentum', beta=0.9)\n",
        "nag_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='nag', beta=0.9)\n",
        "adagrad_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='adagrad')\n",
        "rmsprop_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='rmsprop', beta=0.9)\n",
        "adam_costs = neuron.train(X_train, y_train, num_iterations=100, learning_rate=0.01, optimizer='adam')\n",
        "\n",
        "# Plot the cost curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(gd_costs)), gd_costs, label='Gradient Descent')\n",
        "plt.plot(range(len(mbgd_costs)), mbgd_costs, label='Mini-Batch GD')\n",
        "plt.plot(range(len(momentum_costs)), momentum_costs, label='Momentum-based GD')\n",
        "plt.plot(range(len(nag_costs)), nag_costs, label='Nesterov Accelerated GD')\n",
        "plt.plot(range(len(adagrad_costs)), adagrad_costs, label='AdaGrad')\n",
        "plt.plot(range(len(rmsprop_costs)), rmsprop_costs, label='RMSProp')\n",
        "plt.plot(range(len(adam_costs)), adam_costs, label='Adam')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost Curves for Different Optimization Algorithms')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erItN2jQCaGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}