{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "_I7fX_TxDKwb",
        "outputId": "c67501ec-c742-4730-ea24-a899b647d805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09f355c4173c>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-09f355c4173c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, num_epochs, learning_rate, optimizer)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: FeedForwardNetwork.backward() takes 5 positional arguments but 6 were given"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create dummy classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=3, random_state=42)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Define the FeedForward Neural Network class\n",
        "class FeedForwardNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.weights1 = np.random.randn(input_dim, hidden_dim)\n",
        "        self.bias1 = np.zeros(hidden_dim)\n",
        "        self.weights2 = np.random.randn(hidden_dim, output_dim)\n",
        "        self.bias2 = np.zeros(output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden = np.dot(X, self.weights1) + self.bias1\n",
        "        self.hidden_activation = self.sigmoid(self.hidden)\n",
        "        self.output = np.dot(self.hidden_activation, self.weights2) + self.bias2\n",
        "        self.output_activation = self.softmax(self.output)\n",
        "        return self.output_activation\n",
        "\n",
        "    def backward(self, X, y, output_activation, learning_rate):\n",
        "        batch_size = X.shape[0]\n",
        "        grad_output = output_activation - y\n",
        "        grad_weights2 = np.dot(self.hidden_activation.T, grad_output) / batch_size\n",
        "        grad_bias2 = np.sum(grad_output, axis=0) / batch_size\n",
        "        grad_hidden_activation = np.dot(grad_output, self.weights2.T)\n",
        "        grad_hidden = grad_hidden_activation * self.sigmoid_derivative(self.hidden)\n",
        "        grad_weights1 = np.dot(X.T, grad_hidden) / batch_size\n",
        "        grad_bias1 = np.sum(grad_hidden, axis=0) / batch_size\n",
        "        self.weights2 -= learning_rate * grad_weights2\n",
        "        self.bias2 -= learning_rate * grad_bias2\n",
        "        self.weights1 -= learning_rate * grad_weights1\n",
        "        self.bias1 -= learning_rate * grad_bias1\n",
        "\n",
        "    def train(self, X, y, num_epochs, learning_rate, optimizer):\n",
        "        costs = []\n",
        "        for epoch in range(num_epochs):\n",
        "            output_activation = self.forward(X)\n",
        "            cost = self.cross_entropy_loss(y, output_activation)\n",
        "            costs.append(cost)\n",
        "            self.backward(X, y, output_activation, learning_rate, optimizer)\n",
        "        return costs\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-8\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "# Define the optimization algorithms\n",
        "optimizers = {\n",
        "    'gd': 'Gradient Descent',\n",
        "    'momentum': 'Momentum',\n",
        "    'nag': 'Nesterov Accelerated GD',\n",
        "    'adagrad': 'AdaGrad',\n",
        "    'rmsprop': 'RMSProp',\n",
        "    'adam': 'Adam'\n",
        "}\n",
        "\n",
        "# Define the hyperparameters\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 100\n",
        "output_dim = len(np.unique(y_train))\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Train the model using different optimizers\n",
        "for optimizer_name, optimizer_label in optimizers.items():\n",
        "    # Create the model\n",
        "    model = FeedForwardNetwork(input_dim, hidden_dim, output_dim)\n",
        "    # Train the model\n",
        "    costs = model.train(X_train, y_train_encoded, num_epochs, learning_rate, optimizer_name)\n",
        "    # Evaluate the model\n",
        "    y_train_pred = np.argmax(model.forward(X_train), axis=1)\n",
        "    y_test_pred = np.argmax(model.forward(X_test), axis=1)\n",
        "    train_accuracy = np.mean(y_train == y_train_pred)\n",
        "    test_accuracy = np.mean(y_test == y_test_pred)\n",
        "    # Plot the cost curve\n",
        "    plt.plot(range(1, num_epochs + 1), costs, label=optimizer_label)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost Curve')\n",
        "    plt.legend()\n",
        "\n",
        "    print(f\"Optimizer: {optimizer_label}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"--------------------------------------\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1sB72fpDkRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}